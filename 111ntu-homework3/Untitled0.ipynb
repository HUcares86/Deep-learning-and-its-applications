{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1G6hY8qO-oZfltUvNtIPjN7gtFK3PuQbD","authorship_tag":"ABX9TyNR7VLt2hr5meyVk+5LB3b1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJIer1SkM4qG","executionInfo":{"status":"ok","timestamp":1669703515017,"user_tz":-480,"elapsed":10654,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"8b4520a3-c1e9-4752-8f34-45b52504934e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["## Standard libraries\n","import os\n","import numpy as np\n","import random\n","import math\n","import json\n","from functools import partial\n","\n","## Imports for plotting\n","import matplotlib.pyplot as plt\n","plt.set_cmap('cividis')\n","%matplotlib inline\n","from IPython.display import set_matplotlib_formats\n","set_matplotlib_formats('svg', 'pdf') # For export\n","from matplotlib.colors import to_rgb\n","import matplotlib\n","matplotlib.rcParams['lines.linewidth'] = 2.0\n","import seaborn as sns\n","sns.reset_orig()\n","\n","## tqdm for loading bars\n","from tqdm.notebook import tqdm\n","\n","## PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","import torch.optim as optim\n","\n","## Torchvision\n","import torchvision\n","from torchvision.datasets import CIFAR100\n","from torchvision import transforms\n","\n","# PyTorch Lightning\n","try:\n","    import pytorch_lightning as pl\n","except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n","    !pip install --quiet pytorch-lightning>=1.4\n","    import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n","\n","# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n","DATASET_PATH = \"../data\"\n","# Path to the folder where the pretrained models are saved\n","CHECKPOINT_PATH = \"../saved_models/tutorial6\"\n","\n","# Setting the seed\n","pl.seed_everything(42)\n","\n","# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOnL38qYQbY1","executionInfo":{"status":"ok","timestamp":1669703527863,"user_tz":-480,"elapsed":12851,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"16d0044a-7b79-43f0-8513-26be213e6508"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:lightning_lite.utilities.seed:Global seed set to 42\n"]},{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torchvision\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor\n","from torchvision.utils import make_grid\n","from torch.utils.data.dataloader import DataLoader\n","from torch.utils.data import random_split\n","import pandas as pd\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from sklearn import preprocessing\n","from enum import Enum \n","import copy\n","# import SGD for optimizer\n","from torch.optim import SGD\n","\n","# import Adam for optimizer\n","from torch.optim import Adam\n","\n","# to measure the performance import L1Loss\n","from torch.nn import L1Loss"],"metadata":{"id":"-s05JA-QM9sY","executionInfo":{"status":"ok","timestamp":1669703527864,"user_tz":-480,"elapsed":9,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["training_x = np.load('/content/drive/MyDrive/code/python/projects/ADL/ADL_(IM5062)/111ntu-homework3/training_x.npy')"],"metadata":{"id":"SH3QodSuM9xm","executionInfo":{"status":"ok","timestamp":1669703530093,"user_tz":-480,"elapsed":2237,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["training_x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pjvxXp8NrzS","executionInfo":{"status":"ok","timestamp":1669703530093,"user_tz":-480,"elapsed":28,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"58cd7e86-9b41-4048-9bb1-c55a05e67555"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[14.33333333,  0.33333333,  7.66666667, ...,  2.        ,\n","         0.        ,  0.33333333],\n","       [14.        ,  0.33333333,  9.33333333, ...,  2.        ,\n","        -0.33333333,  0.33333333],\n","       [14.        ,  0.33333333,  5.66666667, ...,  2.        ,\n","         0.        ,  0.66666667],\n","       ...,\n","       [16.        ,  0.        ,  2.        , ...,  1.        ,\n","         0.        ,  3.        ],\n","       [16.        ,  0.        ,  1.        , ...,  2.        ,\n","         0.66666667,  4.66666667],\n","       [15.        ,  0.        ,  0.66666667, ...,  2.        ,\n","         1.33333333,  4.33333333]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["new_training_x = []\n"],"metadata":{"id":"ANlC-taI7eNX","executionInfo":{"status":"ok","timestamp":1669704347160,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["new_training_x "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yfVJO0z7igz","executionInfo":{"status":"ok","timestamp":1669704348978,"user_tz":-480,"elapsed":245,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"26c3ea8f-f560-441e-ffe6-85e4db5f6ba6"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["for i in range(8,training_x.shape[0]-8):\n","  y = []\n","  for j in range(8):\n","    a = training_x[i+j-8]\n","    k = a.tolist()\n","    y.append(k)\n","    # print(y)\n","  new_training_x.append(y)"],"metadata":{"id":"gHJ6k18j3USg","executionInfo":{"status":"ok","timestamp":1669704352369,"user_tz":-480,"elapsed":873,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# new_training_x"],"metadata":{"id":"f-Bkc3gZ3UZu","executionInfo":{"status":"ok","timestamp":1669703530095,"user_tz":-480,"elapsed":20,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# new_training_x[0]"],"metadata":{"id":"YbdKfiwyocWZ","executionInfo":{"status":"ok","timestamp":1669704708905,"user_tz":-480,"elapsed":228,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["len(new_training_x)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjHmApr4OJEm","executionInfo":{"status":"ok","timestamp":1669704403511,"user_tz":-480,"elapsed":242,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"c01101fc-5dd9-4c77-f2f0-be7bd8074ea6"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8744"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["training_y = np.load('/content/drive/MyDrive/code/python/projects/ADL/ADL_(IM5062)/111ntu-homework3/training_y.npy')"],"metadata":{"id":"USZ19WUtM9vA","executionInfo":{"status":"ok","timestamp":1669703530096,"user_tz":-480,"elapsed":18,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# training_y[:17]"],"metadata":{"id":"C1sbrI7SNsKq","executionInfo":{"status":"ok","timestamp":1669704719165,"user_tz":-480,"elapsed":434,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["new_training_y = []"],"metadata":{"id":"TvKeWQ7lDi-v","executionInfo":{"status":"ok","timestamp":1669705163678,"user_tz":-480,"elapsed":400,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["for i in range(8,training_x.shape[0]-8):\n","  y = []\n","  for j in range(8):\n","    a = training_y[i+j][0]\n","    k = a.tolist()\n","    y.append(k)\n","    # print(y)\n","  new_training_y.append(y)"],"metadata":{"id":"AduoK8ZsDjBh","executionInfo":{"status":"ok","timestamp":1669705165380,"user_tz":-480,"elapsed":2,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["new_training_y[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrDzvF7CDv7d","executionInfo":{"status":"ok","timestamp":1669705167185,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"c1717d4d-d460-4b12-ffe6-278064cdcfee"},"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[38.0, 34.0, 33.0, 31.0, 30.0, 33.0, 37.0, 36.0]"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["testing_x = np.load('/content/drive/MyDrive/code/python/projects/ADL/ADL_(IM5062)/111ntu-homework3/testing_x.npy')"],"metadata":{"id":"QtCPRIr4M90a","executionInfo":{"status":"ok","timestamp":1669703531708,"user_tz":-480,"elapsed":605,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["testing_x "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wPHNDc5M924","executionInfo":{"status":"ok","timestamp":1669703531708,"user_tz":-480,"elapsed":8,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"39a412a2-428d-4753-ea5b-2ec3b33579c8"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[15.        ,  0.        ,  0.66666667, ...,  2.        ,\n","         1.33333333,  3.33333333],\n","       [14.66666667,  0.        ,  0.66666667, ...,  2.        ,\n","         0.66666667,  3.33333333],\n","       [14.        ,  0.        ,  0.66666667, ...,  2.        ,\n","         1.        ,  3.        ],\n","       ...,\n","       [16.        ,  0.        ,  0.66666667, ...,  1.        ,\n","         0.33333333,  3.33333333],\n","       [16.        ,  0.        ,  1.        , ...,  1.        ,\n","         0.33333333,  3.        ],\n","       [16.        ,  0.        ,  1.        , ...,  1.        ,\n","         0.33333333,  4.33333333]])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["new_testing_x = []"],"metadata":{"id":"erbBkWu-KGu1","executionInfo":{"status":"ok","timestamp":1669704789203,"user_tz":-480,"elapsed":227,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["for i in range(8,testing_x.shape[0]-8):\n","  y = []\n","  for j in range(8):\n","    a = testing_x[i+j-8]\n","    k = a.tolist()\n","    y.append(k)\n","    # print(y)\n","  new_testing_x.append(y)"],"metadata":{"id":"qOWmvS_6KGxS","executionInfo":{"status":"ok","timestamp":1669704792131,"user_tz":-480,"elapsed":795,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["# new_testing_x[0]"],"metadata":{"id":"cMSdpYm7KGzw","executionInfo":{"status":"ok","timestamp":1669704823409,"user_tz":-480,"elapsed":390,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BJ5dcttUKWFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class testing_Dataset(Dataset):\n","    # This loads the data and converts it, make data rdy\n","    def __init__(self):\n","        # load data\n","        self.df= new_testing_x\n","        # extract labels\n","\n","        # conver to torch dtypes\n","        self.dataset=torch.tensor(self.df).float()\n","\n","    # This returns the total amount of samples in your Dataset\n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    # This returns given an index the i-th sample and label\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]"],"metadata":{"id":"NvLFlLcEf_-q","executionInfo":{"status":"ok","timestamp":1669704842411,"user_tz":-480,"elapsed":415,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["testing_dataset =testing_Dataset()\n","testing_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMxO3YOngi4b","executionInfo":{"status":"ok","timestamp":1669704850673,"user_tz":-480,"elapsed":906,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"9de2cbcd-7008-4d10-9aa1-cbdf8a6eb694"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.testing_Dataset at 0x7fc383dc4910>"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["testing_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSZ9UJyVgmbf","executionInfo":{"status":"ok","timestamp":1669704853230,"user_tz":-480,"elapsed":394,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"f1f14d3a-38a1-4441-a74d-c437ba2887f9"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 15.0000,   0.0000,   0.6667,  11.0000,  12.6667,  42.0000, 159.3333,\n","           0.0000,  64.0000,   8.6667,   2.0000,   1.3333,   3.3333],\n","        [ 14.6667,   0.0000,   0.6667,  11.0000,  12.6667,  41.0000, 161.3333,\n","           0.0000,  63.6667,   7.3333,   2.0000,   0.6667,   3.3333],\n","        [ 14.0000,   0.0000,   0.6667,  10.3333,  12.3333,  41.0000, 149.6667,\n","           0.0000,  64.3333,   6.3333,   2.0000,   1.0000,   3.0000],\n","        [ 14.0000,   0.0000,   0.6667,   8.0000,   9.0000,  40.3333, 135.6667,\n","           0.0000,  65.0000,   5.3333,   1.0000,   0.6667,   3.0000],\n","        [ 14.0000,   0.0000,   0.3333,   7.3333,   8.3333,  39.6667, 119.6667,\n","           0.0000,  65.3333,   4.3333,   1.0000,   0.6667,   3.0000],\n","        [ 13.0000,   0.0000,   0.3333,   8.0000,   8.3333,  38.3333, 108.3333,\n","           0.0000,  64.3333,   4.3333,   1.0000,   0.6667,   2.6667],\n","        [ 13.0000,   0.0000,   0.6667,  10.0000,  11.0000,  34.6667, 102.3333,\n","           0.0000,  64.3333,   5.0000,   1.0000,   0.6667,   3.0000],\n","        [ 13.0000,   0.0000,   1.0000,  11.3333,  13.0000,  31.6667, 101.6667,\n","           0.0000,  62.6667,   5.3333,   1.0000,   0.6667,   3.0000]])"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["batch_size = 100\n","testing_loader = DataLoader(testing_dataset, batch_size, num_workers=2, pin_memory=True)"],"metadata":{"id":"DI7wpnZYaGvZ","executionInfo":{"status":"ok","timestamp":1669704994237,"user_tz":-480,"elapsed":868,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["inp_data = testing_loader.dataset[100]\n","print(\"Input data:\", inp_data)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KA38nMGgq7i","executionInfo":{"status":"ok","timestamp":1669705063323,"user_tz":-480,"elapsed":222,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"d2e38a11-41e3-445d-e954-5343f29912fb"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["Input data: tensor([[14.6667,  0.3333, 14.3333, 21.3333, 35.6667, 10.3333, 32.0000,  0.0000,\n","         85.3333,  1.6667,  2.0000,  0.0000,  0.0000],\n","        [14.6667,  0.0000, 11.0000, 17.6667, 29.0000, 10.6667, 31.3333,  0.0000,\n","         85.6667,  1.3333,  2.0000, -0.3333,  0.0000],\n","        [14.6667,  0.3333, 26.3333, 20.0000, 46.3333,  8.3333, 38.0000,  0.0000,\n","         86.0000,  2.0000,  3.0000,  0.0000,  0.0000],\n","        [15.0000,  0.3333, 30.3333, 24.6667, 55.3333,  7.3333, 44.3333,  0.0000,\n","         84.3333,  4.0000,  2.0000,  0.0000,  0.0000],\n","        [17.0000,  1.0000, 31.3333, 34.3333, 66.0000,  9.0000, 52.0000,  0.0000,\n","         78.0000,  5.6667,  2.0000,  0.0000,  0.3333],\n","        [19.0000,  0.6667, 24.6667, 49.0000, 74.0000, 16.0000, 65.0000,  0.0000,\n","         67.0000,  5.3333,  2.0000, -0.3333,  0.3333],\n","        [21.0000,  0.6667, 15.6667, 46.3333, 62.0000, 30.0000, 68.0000,  0.0000,\n","         59.6667,  4.0000,  2.0000,  0.0000,  0.6667],\n","        [22.6667,  0.6667,  6.3333, 32.0000, 38.3333, 43.6667, 57.6667,  0.0000,\n","         55.0000,  3.3333,  2.0000,  0.0000, -0.3333]])\n"]}]},{"cell_type":"code","source":["class TrainingDataset(Dataset):\n","    # This loads the data and converts it, make data rdy\n","    def __init__(self):\n","        # load data\n","        self.df= new_training_x[:8000]\n","        # extract labels\n","        self.df_labels= new_training_y[:8000]\n","        # conver to torch dtypes\n","        self.dataset=torch.tensor(self.df).float()\n","\n","        self.labels=torch.tensor(self.df_labels).long()\n","    \n","    # This returns the total amount of samples in your Dataset\n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    # This returns given an index the i-th sample and label\n","    def __getitem__(self, idx):\n","        return self.dataset[idx],self.labels[idx]"],"metadata":{"id":"PLABfSgNM95W","executionInfo":{"status":"ok","timestamp":1669705842275,"user_tz":-480,"elapsed":230,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["class ValidDataset(Dataset):\n","    # This loads the data and converts it, make data rdy\n","    def __init__(self):\n","        # load data\n","        self.df= new_training_x[8000:]\n","        # extract labels\n","        self.df_labels= new_training_y[8000:]\n","        # conver to torch dtypes\n","        self.dataset=torch.tensor(self.df).float()\n","\n","        self.labels=torch.tensor(self.df_labels).long()\n","    \n","    # This returns the total amount of samples in your Dataset\n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    # This returns given an index the i-th sample and label\n","    def __getitem__(self, idx):\n","        return self.dataset[idx],self.labels[idx]"],"metadata":{"id":"fvd_M7TYNTJy","executionInfo":{"status":"ok","timestamp":1669705844069,"user_tz":-480,"elapsed":2,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["train_ds = TrainingDataset()\n","val_ds = ValidDataset()"],"metadata":{"id":"1zu3CyvYM98J","executionInfo":{"status":"ok","timestamp":1669705845899,"user_tz":-480,"elapsed":240,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["print(len(train_ds), len(val_ds))\n","\n","from torch.utils.data import DataLoader\n","\n","batch_size = 100\n","\n","train_loader = DataLoader(train_ds,batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","val_loader = DataLoader(val_ds, batch_size, num_workers=2, pin_memory=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Civd1ImYM-Bh","executionInfo":{"status":"ok","timestamp":1669705862635,"user_tz":-480,"elapsed":3,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"34e366e1-9eee-46b1-9f74-eabbffb73d1a"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["8000 744\n"]}]},{"cell_type":"code","source":["inp_data, ans = train_loader.dataset[0]\n","print(\"Input data:\", inp_data)\n","print(\"Labels:    \", ans)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KylGbA9YM-EF","executionInfo":{"status":"ok","timestamp":1669705868081,"user_tz":-480,"elapsed":386,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"554c37ed-d98e-4c59-f284-05f0e2bfc868"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["Input data: tensor([[14.3333,  0.3333,  7.6667, 37.0000, 44.6667, 12.3333, 53.0000,  0.0000,\n","         76.3333,  1.6667,  2.0000,  0.0000,  0.3333],\n","        [14.0000,  0.3333,  9.3333, 34.3333, 43.6667, 14.3333, 53.6667,  0.0000,\n","         78.0000,  4.6667,  2.0000, -0.3333,  0.3333],\n","        [14.0000,  0.3333,  5.6667, 36.6667, 42.3333, 14.6667, 55.0000,  0.0000,\n","         77.3333,  4.0000,  2.0000,  0.0000,  0.6667],\n","        [13.6667,  0.6667, 16.6667, 37.3333, 54.3333, 12.3333, 57.3333,  0.0000,\n","         78.6667,  3.0000,  2.0000, -0.3333,  0.3333],\n","        [13.3333,  0.3333,  6.3333, 24.3333, 30.6667, 21.3333, 54.3333,  0.0000,\n","         80.3333,  2.0000,  2.0000,  0.3333,  0.6667],\n","        [13.3333,  0.3333, 12.0000, 24.6667, 37.3333, 22.0000, 49.6667,  0.0000,\n","         81.0000,  2.6667,  2.0000,  0.0000,  0.6667],\n","        [13.3333,  0.3333, 28.0000, 27.6667, 55.6667, 21.0000, 49.3333,  0.0000,\n","         81.6667,  6.6667,  2.0000, -0.3333,  0.3333],\n","        [13.6667,  0.3333, 22.6667, 30.0000, 53.0000, 18.6667, 53.3333,  0.0000,\n","         80.6667,  8.0000,  2.0000, -0.3333,  0.3333]])\n","Labels:     tensor([38, 34, 33, 31, 30, 33, 37, 36])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pEwwVd2Segft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gqVMMRptegh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n","    attn_logits = attn_logits / math.sqrt(d_k)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = F.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention"],"metadata":{"id":"pPbnYVMjeAyM","executionInfo":{"status":"ok","timestamp":1669705886998,"user_tz":-480,"elapsed":233,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["class MultiheadAttention(nn.Module):\n","\n","    def __init__(self, input_dim, embed_dim, num_heads):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n","\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","\n","        # Stack all weight matrices 1...h together for efficiency\n","        # Note that in many implementations you see \"bias=False\" which is optional\n","        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n","        self.o_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        # Original Transformer initialization, see PyTorch documentation\n","        nn.init.xavier_uniform_(self.qkv_proj.weight)\n","        self.qkv_proj.bias.data.fill_(0)\n","        nn.init.xavier_uniform_(self.o_proj.weight)\n","        self.o_proj.bias.data.fill_(0)\n","\n","    def forward(self, x, mask=None, return_attention=False):\n","        batch_size, seq_length, _ = x.size()\n","        qkv = self.qkv_proj(x)\n","\n","        # Separate Q, K, V from linear output\n","        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n","        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n","        q, k, v = qkv.chunk(3, dim=-1)\n","\n","        # Determine value outputs\n","        values, attention = scaled_dot_product(q, k, v, mask=mask)\n","        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n","        values = values.reshape(batch_size, seq_length, self.embed_dim)\n","        o = self.o_proj(values)\n","\n","        if return_attention:\n","            return o, attention\n","        else:\n","            return o"],"metadata":{"id":"fl1ezgKDeA09","executionInfo":{"status":"ok","timestamp":1669705888657,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["class EncoderBlock(nn.Module):\n","\n","    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n","        \"\"\"\n","        Inputs:\n","            input_dim - Dimensionality of the input\n","            num_heads - Number of heads to use in the attention block\n","            dim_feedforward - Dimensionality of the hidden layer in the MLP\n","            dropout - Dropout probability to use in the dropout layers\n","        \"\"\"\n","        super().__init__()\n","\n","        # Attention layer\n","        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n","\n","        # Two-layer MLP\n","        self.linear_net = nn.Sequential(\n","            nn.Linear(input_dim, dim_feedforward),\n","            nn.Dropout(dropout),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(dim_feedforward, input_dim)\n","        )\n","\n","        # Layers to apply in between the main layers\n","        self.norm1 = nn.LayerNorm(input_dim)\n","        self.norm2 = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        # Attention part\n","        attn_out = self.self_attn(x, mask=mask)\n","        x = x + self.dropout(attn_out)\n","        x = self.norm1(x)\n","\n","        # MLP part\n","        linear_out = self.linear_net(x)\n","        x = x + self.dropout(linear_out)\n","        x = self.norm2(x)\n","\n","        return x"],"metadata":{"id":"hcAj9mCfeA39","executionInfo":{"status":"ok","timestamp":1669705889705,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","\n","    def __init__(self, num_layers, **block_args):\n","        super().__init__()\n","        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n","\n","    def forward(self, x, mask=None):\n","        for l in self.layers:\n","            x = l(x, mask=mask)\n","        return x\n","\n","    def get_attention_maps(self, x, mask=None):\n","        attention_maps = []\n","        for l in self.layers:\n","            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n","            attention_maps.append(attn_map)\n","            x = l(x)\n","        return attention_maps"],"metadata":{"id":"M54Qv2OseA7F","executionInfo":{"status":"ok","timestamp":1669705891188,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, max_len=5000):\n","        \"\"\"\n","        Inputs\n","            d_model - Hidden dimensionality of the input.\n","            max_len - Maximum length of a sequence to expect.\n","        \"\"\"\n","        super().__init__()\n","\n","        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","\n","        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n","        # Used for tensors that need to be on the same device as the module.\n","        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n","        self.register_buffer('pe', pe, persistent=False)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return x"],"metadata":{"id":"OJHHHO6reA-G","executionInfo":{"status":"ok","timestamp":1669705892196,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n","\n","    def __init__(self, optimizer, warmup, max_iters):\n","        self.warmup = warmup\n","        self.max_num_iters = max_iters\n","        super().__init__(optimizer)\n","\n","    def get_lr(self):\n","        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n","        return [base_lr * lr_factor for base_lr in self.base_lrs]\n","\n","    def get_lr_factor(self, epoch):\n","        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n","        if epoch <= self.warmup:\n","            lr_factor *= epoch * 1.0 / self.warmup\n","        return lr_factor"],"metadata":{"id":"czqTe3SseA_M","executionInfo":{"status":"ok","timestamp":1669705893899,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["class TransformerPredictor(pl.LightningModule):\n","\n","    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n","        \"\"\"\n","        Inputs:\n","            input_dim - Hidden dimensionality of the input\n","            model_dim - Hidden dimensionality to use inside the Transformer\n","            num_classes - Number of classes to predict per sequence element\n","            num_heads - Number of heads to use in the Multi-Head Attention blocks\n","            num_layers - Number of encoder blocks to use.\n","            lr - Learning rate in the optimizer\n","            warmup - Number of warmup steps. Usually between 50 and 500\n","            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n","            dropout - Dropout to apply inside the model\n","            input_dropout - Dropout to apply on the input features\n","        \"\"\"\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self._create_model()\n","\n","    def _create_model(self):\n","        # Input dim -> Model dim\n","        self.input_net = nn.Sequential(\n","            nn.Dropout(self.hparams.input_dropout),\n","            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n","        )\n","        # Positional encoding for sequences\n","        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n","        # Transformer\n","        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n","                                              input_dim=self.hparams.model_dim,\n","                                              dim_feedforward=2*self.hparams.model_dim,\n","                                              num_heads=self.hparams.num_heads,\n","                                              dropout=self.hparams.dropout)\n","        # Output classifier per sequence lement\n","        self.output_net = nn.Sequential(\n","            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n","            nn.LayerNorm(self.hparams.model_dim),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(self.hparams.dropout),\n","            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n","        )\n","\n","    def forward(self, x, mask=None, add_positional_encoding=True):\n","        \"\"\"\n","        Inputs:\n","            x - Input features of shape [Batch, SeqLen, input_dim]\n","            mask - Mask to apply on the attention outputs (optional)\n","            add_positional_encoding - If True, we add the positional encoding to the input.\n","                                      Might not be desired for some tasks.\n","        \"\"\"\n","        x = self.input_net(x)\n","        if add_positional_encoding:\n","            x = self.positional_encoding(x)\n","        x = self.transformer(x, mask=mask)\n","        x = self.output_net(x)\n","        return x\n","\n","    @torch.no_grad()\n","    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n","        \"\"\"\n","        Function for extracting the attention matrices of the whole Transformer for a single batch.\n","        Input arguments same as the forward pass.\n","        \"\"\"\n","        x = self.input_net(x)\n","        if add_positional_encoding:\n","            x = self.positional_encoding(x)\n","        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n","        return attention_maps\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n","\n","        # Apply lr scheduler per step\n","        lr_scheduler = CosineWarmupScheduler(optimizer,\n","                                             warmup=self.hparams.warmup,\n","                                             max_iters=self.hparams.max_iters)\n","        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n","\n","    def training_step(self, batch, batch_idx):\n","        raise NotImplementedError\n","\n","    def validation_step(self, batch, batch_idx):\n","        raise NotImplementedError\n","\n","    def test_step(self, batch, batch_idx):\n","        raise NotImplementedError"],"metadata":{"id":"2wPTu0XEdsBS","executionInfo":{"status":"ok","timestamp":1669705895583,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["class PM2_5_Predictor(TransformerPredictor):\n","\n","    def _calculate_loss(self, batch, mode=\"train\"):\n","        # Fetch data and transform categories to one-hot vectors\n","        inp_data, labels = batch\n","        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n","\n","        # Perform prediction and calculate loss and accuracy\n","        preds = self.forward(inp_data, add_positional_encoding=True)\n","        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n","        acc = (preds.argmax(dim=-1) == labels).float().mean()\n","\n","        # Logging\n","        self.log(f\"{mode}_loss\", loss)\n","        self.log(f\"{mode}_acc\", acc)\n","        return loss, acc\n","\n","    def training_step(self, batch, batch_idx):\n","        loss, _ = self._calculate_loss(batch, mode=\"train\")\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        _ = self._calculate_loss(batch, mode=\"val\")\n","\n","    def test_step(self, batch, batch_idx):\n","        _ = self._calculate_loss(batch, mode=\"test\")"],"metadata":{"id":"dkryy7M7M-Gj","executionInfo":{"status":"ok","timestamp":1669705898206,"user_tz":-480,"elapsed":1,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["def train_PM2_5(**kwargs):\n","    # Create a PyTorch Lightning trainer with the generation callback\n","    root_dir = os.path.join(CHECKPOINT_PATH, \"PM2_5_Task\")\n","    os.makedirs(root_dir, exist_ok=True)\n","    trainer = pl.Trainer(default_root_dir=root_dir,\n","                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n","                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n","                         devices=1,\n","                         max_epochs=10,\n","                         gradient_clip_val=5)\n","    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n","\n","    # Check whether pretrained model exists. If yes, load it and skip training\n","    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"PM2_5_Task.ckpt\")\n","    if os.path.isfile(pretrained_filename):\n","        print(\"Found pretrained model, loading...\")\n","        model = PM2_5_Predictor.load_from_checkpoint(pretrained_filename)\n","    else:\n","        model = PM2_5_Predictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n","        trainer.fit(model, train_loader, val_loader)\n","\n","    # Test best model on validation and test set\n","    val_result = trainer.test(model, val_loader, verbose=False)\n","    test_result = trainer.test(model, testing_loader, verbose=False)\n","    result = {\"val_acc\": val_result[0][\"test_acc\"]}\n","\n","    model = model.to(device)\n","    return model, result"],"metadata":{"id":"SSCDNYbkM-JK","executionInfo":{"status":"ok","timestamp":1669705910292,"user_tz":-480,"elapsed":434,"user":{"displayName":"cares HU","userId":"02624893690234323438"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["PM2_5_model, PM2_5_result = train_PM2_5(input_dim=train_loader.dataset.num_categories,\n","                                              model_dim=13*2,\n","                                              num_heads=1,\n","                                              num_classes=train_loader.dataset.num_categories,\n","                                              num_layers=1,\n","                                              dropout=0.0,\n","                                              lr=5e-4,\n","                                              warmup=50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"LDoMbzt-M-L4","executionInfo":{"status":"error","timestamp":1669705913548,"user_tz":-480,"elapsed":364,"user":{"displayName":"cares HU","userId":"02624893690234323438"}},"outputId":"eca73039-ce0c-4085-ad38-cc90fd7e2ec3"},"execution_count":120,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-120-2a9b8dae5252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m PM2_5_model, PM2_5_result = train_PM2_5(input_dim=train_loader.dataset.num_categories,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                               \u001b[0mmodel_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                               \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                               \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_categories\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                               \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'TrainingDataset' object has no attribute 'num_categories'"]}]},{"cell_type":"code","source":["print(f\"Val accuracy:  {(100.0 * reverse_result['val_acc']):4.2f}%\")\n","print(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")"],"metadata":{"id":"c1UVBJE7M-Oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gCMEVhMBM-Q6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vTzxKA_gM-Tc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nqa1GBaFM-V0"},"execution_count":null,"outputs":[]}]}